## Pythonç¼–ç¨‹æ¼”ç¤º
$ import scanpy as sc
$ import pandas as pd
$ import numpy as np
$ import matplotlib.pyplot as plt

æ¨¡å—å¯¼å…¥å®Œæˆ
- scanpy: 1.9.3 (å•ç»†èƒåˆ†æ)
- pandas: 2.0.3 (æ•°æ®å¤„ç†)
- numpy: 1.24.3 (æ•°å€¼è®¡ç®—)  
- matplotlib: 3.7.2 (å¯è§†åŒ–)

$ # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
$ adata = sc.read_h5ad('data.h5ad')
$ print(f"æ•°æ®ç»´åº¦: {adata.shape}")
æ•°æ®ç»´åº¦: (3000, 20000)

$ # è®¡ç®—è´¨æ§æŒ‡æ ‡
$ adata.var['mt'] = adata.var_names.str.startswith('MT-')
$ sc.pp.calculate_qc_metrics(adata, percent_top=None, 
                            log1p=False, inplace=True)
$ print("è´¨æ§æŒ‡æ ‡è®¡ç®—å®Œæˆ")
è´¨æ§æŒ‡æ ‡è®¡ç®—å®Œæˆ

$ # å¯è§†åŒ–è´¨æ§æŒ‡æ ‡
$ sc.pl.violin(adata, ['n_genes_by_counts', 'total_counts', 
                      'pct_counts_mt'],
               jitter=0.4, multi_panel=True)
æ­£åœ¨ç”Ÿæˆè´¨æ§å°æç´å›¾...
âœ“ å›¾å·²ä¿å­˜: figures/qc_violin.pdf

## Rè¯­è¨€é›†æˆæ¼”ç¤º
$ # ç°åœ¨åˆ‡æ¢åˆ°Rè¿›è¡ŒSeuratåˆ†æ
$ library(Seurat)
$ library(dplyr)

Loading required package: Seurat
Loading required package: dplyr
âœ“ RåŒ…åŠ è½½å®Œæˆ

$ # åˆ›å»ºSeuratå¯¹è±¡
$ pbmc <- CreateSeuratObject(counts = pbmc.data, 
                            project = "pbmc3k", 
                            min.cells = 3, 
                            min.features = 200)
An object of class Seurat 
13714 features across 2700 samples within 1 assay 
Active assay: RNA (13714 features, 0 variable features)

$ # æ•°æ®å½’ä¸€åŒ–å’Œç¼©æ”¾
$ pbmc <- NormalizeData(pbmc)
$ pbmc <- FindVariableFeatures(pbmc, selection.method = "vst")
$ pbmc <- ScaleData(pbmc, features = rownames(pbmc))

Normalizing layer: counts
Finding variable features for layer counts
Centering and scaling data matrix

$ # ä¸»æˆåˆ†åˆ†æ
$ pbmc <- RunPCA(pbmc, features = VariableFeatures(object = pbmc))
Computing nearest neighbor graph
Computing SNN

$ # èšç±»åˆ†æ
$ pbmc <- FindNeighbors(pbmc, dims = 1:10)  
$ pbmc <- FindClusters(pbmc, resolution = 0.5)
Modularity Optimizer version 1.3.0 by Ludo Waltman and Nees Jan van Eck

Found 9 clusters

$ # UMAPé™ç»´å¯è§†åŒ–
$ pbmc <- RunUMAP(pbmc, dims = 1:10)
$ DimPlot(pbmc, reduction = "umap")
âœ“ UMAPå›¾å·²ç”Ÿæˆ

## Juliaé«˜æ€§èƒ½è®¡ç®—æ¼”ç¤º
$ # åˆ‡æ¢åˆ°Juliaè¿›è¡Œæ•°å€¼è®¡ç®—
$ using LinearAlgebra, Statistics
$ using Random, Distributions

Juliaç¯å¢ƒå·²åˆå§‹åŒ–
- LinearAlgebra: çº¿æ€§ä»£æ•°è®¡ç®—
- Statistics: ç»Ÿè®¡åˆ†æ
- é«˜æ€§èƒ½æ•°å€¼è®¡ç®—å·²å°±ç»ª

$ # ç”Ÿæˆå¤§è§„æ¨¡æ¨¡æ‹Ÿæ•°æ®
$ n_cells = 10000
$ n_genes = 5000  
$ expression_matrix = rand(Normal(0, 1), n_genes, n_cells)
$ println("æ¨¡æ‹Ÿè¡¨è¾¾çŸ©é˜µ: $(size(expression_matrix))")
æ¨¡æ‹Ÿè¡¨è¾¾çŸ©é˜µ: (5000, 10000)

$ # é«˜æ•ˆè®¡ç®—åŸºå› é—´ç›¸å…³æ€§
$ @time cor_matrix = cor(expression_matrix)
  0.892847 seconds (12 allocations: 763.016 MiB)
5000Ã—5000 Matrix{Float64}

$ # ä¸»æˆåˆ†åˆ†æ
$ using MultivariateStats
$ @time pca_result = fit(PCA, expression_matrix; maxoutdim=50)
  0.234156 seconds (45 allocations: 228.882 MiB)
PCA(indim = 5000, outdim = 50, principalratio = 0.9876)

$ # èšç±»åˆ†æ
$ using Clustering
$ @time clusters = kmeans(pca_result.proj, 8)
  0.045123 seconds (1,245 allocations: 4.321 MiB)  
âœ“ Juliaé«˜æ€§èƒ½è®¡ç®—å±•ç¤ºå®Œæˆ

## æ··åˆç¼–ç¨‹æ¼”ç¤º
$ # Python: åŠ è½½å®é™…æ•°æ®
$ import scanpy as sc
$ adata = sc.read_h5ad('single_cell_data.h5ad')
$ X_python = adata.X.toarray()
$ print(f"Pythonæ•°æ®: {X_python.shape}")
Pythonæ•°æ®: (3000, 2000)

$ # R: è¿›è¡Œå·®å¼‚è¡¨è¾¾åˆ†æ  
$ library(DESeq2)
$ count_matrix <- py$X_python
$ dds <- DESeqDataSetFromMatrix(
    countData = count_matrix,
    colData = data.frame(condition = rep(c("A", "B"), each = 1500)),
    design = ~ condition
  )
$ dds <- DESeq(dds)
$ results <- results(dds)
âœ“ DESeq2å·®å¼‚åˆ†æå®Œæˆ

$ # Julia: é«˜æ€§èƒ½ç»Ÿè®¡æ£€éªŒ
$ using HypothesisTests, StatsBase
$ python_data = py"X_python"
$ group1 = python_data[:, 1:1500]
$ group2 = python_data[:, 1501:3000]
$ 
$ # å¯¹æ¯ä¸ªåŸºå› è¿›è¡Œtæ£€éªŒ
$ pvals = [pvalue(UnequalVarianceTTest(group1[i,:], group2[i,:])) 
          for i in 1:2000]
$ println("å®Œæˆ $(length(pvals)) ä¸ªåŸºå› çš„ç»Ÿè®¡æ£€éªŒ")
å®Œæˆ 2000 ä¸ªåŸºå› çš„ç»Ÿè®¡æ£€éªŒ

$ # å›åˆ°Python: å¯è§†åŒ–ç»“æœ
$ import matplotlib.pyplot as plt
$ import seaborn as sns
$ # è·å–Juliaè®¡ç®—çš„på€¼
$ pvals_py = julia.eval('pvals')
$ # ç»˜åˆ¶på€¼åˆ†å¸ƒç›´æ–¹å›¾
$ plt.figure(figsize=(10, 6))
$ plt.hist(pvals_py, bins=50, alpha=0.7)
$ plt.xlabel('P-values')
$ plt.ylabel('Frequency')  
$ plt.title('Distribution of P-values from Julia t-tests')
$ plt.savefig('pvalue_distribution.pdf')
âœ“ æ··åˆç¼–ç¨‹åˆ†æå®Œæˆï¼Œç»“æœå·²å¯è§†åŒ–

## Shellå‘½ä»¤é›†æˆæ¼”ç¤º
$ # ä½¿ç”¨ç”Ÿç‰©ä¿¡æ¯å­¦å·¥å…·è¿›è¡Œåºåˆ—åˆ†æ
$ fastp -i sample_R1.fastq -I sample_R2.fastq \
        -o clean_R1.fastq -O clean_R2.fastq \
        --html fastp_report.html
Read1 before filtering:
total reads: 1000000
total bases: 150000000
Q20 bases: 142500000 (95.00%)
Q30 bases: 135000000 (90.00%)

âœ“ è´¨æ§å®Œæˆï¼Œç”Ÿæˆæ¸…ç†åçš„æµ‹åºæ•°æ®

$ # STARæ¯”å¯¹åˆ°å‚è€ƒåŸºå› ç»„
$ STAR --runMode alignReads \
       --genomeDir /ref/genome_index \
       --readFilesIn clean_R1.fastq clean_R2.fastq \
       --outFileNamePrefix sample_ \
       --outSAMtype BAM SortedByCoordinate
Started mapping
Mapping speed: 5.2M reads/minute  
Uniquely mapped reads: 85.3%
âœ“ åºåˆ—æ¯”å¯¹å®Œæˆ

$ # ä½¿ç”¨kallistoè¿›è¡Œè½¬å½•æœ¬å®šé‡
$ kallisto quant -i transcripts.idx \
                 -o kallisto_output \
                 clean_R1.fastq clean_R2.fastq
[quant] fragment length distribution will be estimated from the data
[quant] fragment length distribution has mean: 180, sd: 20  
[quant] running in paired-end mode
[quant] processed 1,000,000 reads, 950,000 pseudoaligned
âœ“ è½¬å½•æœ¬å®šé‡å®Œæˆ

## é”™è¯¯å¤„ç†å’Œè°ƒè¯•æ¼”ç¤º
$ # æ•…æ„äº§ç”Ÿä¸€ä¸ªé”™è¯¯
$ result = undefined_variable + 5
NameError: name 'undefined_variable' is not defined

ğŸ” æ™ºèƒ½é”™è¯¯è¯Šæ–­:
- é”™è¯¯ç±»å‹: NameError  
- é—®é¢˜: å˜é‡ 'undefined_variable' æœªå®šä¹‰
- å»ºè®®ä¿®å¤:
  1. æ£€æŸ¥å˜é‡åæ‹¼å†™æ˜¯å¦æ­£ç¡®
  2. ç¡®è®¤å˜é‡å·²åœ¨ä½¿ç”¨å‰å®šä¹‰
  3. å¯èƒ½çš„ä¿®å¤: undefined_variable = 0

$ # ä¿®å¤é”™è¯¯
$ defined_variable = 10
$ result = defined_variable + 5
$ print(f"è®¡ç®—ç»“æœ: {result}")
è®¡ç®—ç»“æœ: 15

$ # è°ƒè¯•æ¨¡å¼æ¼”ç¤º
$ %debug
> /path/to/analysis.py(45)calculate_metrics()
     44     for i in range(len(data)):
---> 45         metric = data[i] / total
     46         metrics.append(metric)

ipdb> p data[i]
125.6
ipdb> p total  
0
ipdb> # å‘ç°é™¤é›¶é”™è¯¯çš„åŸå› 
ipdb> c

ğŸ› è°ƒè¯•ä¿¡æ¯:
- åœ¨ç¬¬45è¡Œå‘ç°é™¤é›¶é”™è¯¯
- totalå˜é‡å€¼ä¸º0ï¼Œå¯¼è‡´é™¤æ³•è¿ç®—å¤±è´¥
- å»ºè®®æ·»åŠ æ¡ä»¶æ£€æŸ¥é¿å…é™¤é›¶

$ # ä¼˜åŒ–åçš„ä»£ç 
$ def safe_calculate_metrics(data, total):
$     if total == 0:
$         return [0] * len(data)
$     return [x / total for x in data]
$ 
$ metrics = safe_calculate_metrics(data, total)
âœ“ é”™è¯¯å·²ä¿®å¤ï¼Œä»£ç è¿è¡Œæ­£å¸¸

## æ€§èƒ½ä¼˜åŒ–æ¼”ç¤º
$ # ä½¿ç”¨å‘é‡åŒ–æ“ä½œæ›¿ä»£å¾ªç¯
$ # åŸå§‹æ…¢é€Ÿä»£ç 
$ import time
$ start = time.time()
$ result_slow = []
$ for i in range(100000):
$     result_slow.append(np.sin(i * 0.01))
$ slow_time = time.time() - start
$ print(f"å¾ªç¯æ–¹æ³•è€—æ—¶: {slow_time:.4f}ç§’")
å¾ªç¯æ–¹æ³•è€—æ—¶: 0.3247ç§’

$ # ä¼˜åŒ–çš„å‘é‡åŒ–ä»£ç   
$ start = time.time()
$ x = np.arange(100000) * 0.01
$ result_fast = np.sin(x)
$ fast_time = time.time() - start
$ print(f"å‘é‡åŒ–æ–¹æ³•è€—æ—¶: {fast_time:.4f}ç§’")
å‘é‡åŒ–æ–¹æ³•è€—æ—¶: 0.0023ç§’

$ print(f"æ€§èƒ½æå‡: {slow_time/fast_time:.1f}å€")
æ€§èƒ½æå‡: 141.2å€

ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:
- ä½¿ç”¨NumPyå‘é‡åŒ–æ“ä½œæ›¿ä»£Pythonå¾ªç¯
- é¿å…é‡å¤è®¡ç®—ï¼Œä½¿ç”¨ç¼“å­˜æœºåˆ¶
- å¯¹å¤§æ•°æ®é›†è€ƒè™‘ä½¿ç”¨Daskæˆ–å¤šçº¿ç¨‹å¹¶è¡Œ
- å†…å­˜å¯†é›†å‹æ“ä½œè€ƒè™‘ä½¿ç”¨Juliaæˆ–Cython